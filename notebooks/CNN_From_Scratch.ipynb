{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM89us8ag9d8gkt4EUqDI1V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DoubleCyclone/CNN-From-Scratch/blob/main/notebooks/CNN_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, I will build a **Convolutional Neural Network Model** with **NumPy** and train it on the [CIFAR-10 dataset](https://www.kaggle.com/c/cifar-10/). Hoping to achieve a test accuracy of 70%.\n",
        "\n",
        "# 1 - Setup and Data Acquisition\n",
        "I will start this menacing task by importing the packages required as usual."
      ],
      "metadata": {
        "id": "UQrdUOVeFvOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "sKzKx7pIHDaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the package imports, the first thing I have to do is load the dataset. I will also split the training data into Training and Validation sets so that while training the model we can use the Validation set for a more accurate metrics summary of the model. I know that the dataset has 50000 training and 10000 test samples so I will divide it to 40000 training, 10000 validation and 10000 test samples instead."
      ],
      "metadata": {
        "id": "FMpDKAaeHXUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the train and test datasets\n",
        "dataset_full = datasets.CIFAR10(root=\"data\", train=True, download=True)\n",
        "dataset_test = datasets.CIFAR10(root=\"data\", train=False, download=True)\n",
        "\n",
        "# Split the training set into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(dataset_full.data, dataset_full.targets, test_size=0.2, stratify=dataset_full.targets, random_state=42)\n",
        "X_test = dataset_test.data\n",
        "y_test = dataset_test.targets\n",
        "\n",
        "print(f\"Training data : {X_train.shape}, labels : {len(y_train)}\")\n",
        "print(f\"Validation data : {X_val.shape}, labels : {len(y_val)}\")\n",
        "print(f\"Test data : {X_test.shape}, labels : {len(y_test)}\")\n",
        "print(f\"Classes : {dataset_full.classes}, count {len(dataset_full.classes)}\")\n",
        "\n",
        "# Transpose all data to fit PyTorch standards\n",
        "X_train = np.transpose(X_train, (0, 3, 1, 2))\n",
        "X_val = np.transpose(X_val, (0, 3, 1, 2))\n",
        "X_test = np.transpose(X_test, (0, 3, 1, 2))\n",
        "\n",
        "print(f\"Training data transposed : {X_train.shape}, labels : {len(y_train)}\")\n",
        "print(f\"Validation data transposed : {X_val.shape}, labels : {len(y_val)}\")\n",
        "print(f\"Test data transposed : {X_test.shape}, labels : {len(y_test)}\")"
      ],
      "metadata": {
        "id": "ELmFnvwUIGkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One sample's shape is 32x32x3 which represents the size of the image and the channels in that image. Channel count being 3 here means that the image is composed of Red, Green and Blue values on top of each other (RGB). If it was comprised of a single channel, that would have meant that the image was monochromatic. (Like in the [MNIST Digit Classification dataset](https://www.kaggle.com/code/hojjatk/read-mnist-dataset)). Also, there are 10 classes in the dataset comprised of some animal species and vehicles.\n",
        "\n",
        "Let's visualize a few images to get an idea about how they look."
      ],
      "metadata": {
        "id": "PjDfL5WKO-J_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_random(X, size, amount) :\n",
        "  # Pick random indexes\n",
        "  rng = np.random.default_rng()\n",
        "  random_idx = rng.choice(len(X), size=amount, replace=False)\n",
        "\n",
        "  plt.figure(figsize=(amount * 3, 2.5)) # Adjust figure size for better visualization\n",
        "  for j, i in enumerate(random_idx) :\n",
        "    # Transpose image from (Channels, Height, Width) to (Height, Width, Channels)\n",
        "    # before creating PIL Image object\n",
        "    img_data = np.transpose(X[i], (1, 2, 0))\n",
        "    # Resize image using LANCZOS filter for high-quality upscaling\n",
        "    img = Image.fromarray(img_data).resize((size, size), resample=Image.Resampling.LANCZOS)\n",
        "    plt.subplot(1, amount, j + 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Index: {i}, Label : {dataset_full.classes[y_train[i]]}\") # Set the index and class as the title\n",
        "    plt.axis('off') # Hide axes for cleaner image display\n",
        "  plt.tight_layout() # Adjust subplot parameters for a tight layout\n",
        "  plt.show() # Display the plot"
      ],
      "metadata": {
        "id": "rKut1A2kSxbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_random(X_train, 128, 5)"
      ],
      "metadata": {
        "id": "6pDlJTj3Vh-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the images are really small originally, upscaling them leads to low res visualizations but they are still understandable.\n",
        "\n",
        "We use CNNs instead of NNs for image classifications because they keep information about the pixels' neighbors as well as the pixels themselves, learning about patterns by themselves.\n",
        "\n",
        "For my simple model, I will have the architecture built like ```Conv(32 filters) -> MaxPooling -> Conv(64 filters) -> MaxPooling -> Dense(128) -> Dense(10)``` and I will use filters of size 3x3 with no stride or padding for the convolution and a window size of 2 for the **Max Pooling**.\n",
        "\n",
        "# 2 - Building CNN Mathematical Components\n",
        "At first, I need to scale the data from the range 0-255 to a smaller range like 0-1 to avoid the gradient explosion problem.\n"
      ],
      "metadata": {
        "id": "GmAg7bnXaQ6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaledown(data) :\n",
        "  return data / 255.0"
      ],
      "metadata": {
        "id": "iXmCdH3Po3Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_images = scaledown(X_train[:10])\n",
        "print(test_images.shape)\n",
        "print(test_images[0])"
      ],
      "metadata": {
        "id": "X2WeFofepG3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems to work so let's run it on all the data."
      ],
      "metadata": {
        "id": "ksQpXEHnQpL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled = scaledown(X_train)\n",
        "X_val_scaled = scaledown(X_val)\n",
        "X_test_scaled = scaledown(X_test)"
      ],
      "metadata": {
        "id": "1blE8B0eQrZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then I need to convert the images from 3 dimensional arrays to 2 dimensional arrays by getting rid of the channel dimension and just concatenating the channels together in such a way that every column represents all the information gathered from one position from the sliding window technique (1 column = kernel size x channel size amount of data). This approach is called **im2col**."
      ],
      "metadata": {
        "id": "OVzeuKPco1sA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def im2col(images, kernels) :\n",
        "  _, kernel_channels, kernel_h, kernel_w = kernels.shape  # Get kernel dimensions\n",
        "\n",
        "  B, C_img, H_img, W_img = images.shape # Get image dimensions\n",
        "\n",
        "  # Calculate output dimensions\n",
        "  output_h = H_img - kernel_h + 1\n",
        "  output_w = W_img - kernel_w + 1\n",
        "\n",
        "  # The number of patches for each image\n",
        "  num_patches = output_h * output_w\n",
        "\n",
        "  # The size of each flattened patch (image_channels * kernel_height * kernel_width)\n",
        "  patch_size = C_img * kernel_h * kernel_w # C_img here refers to the input image's channel count\n",
        "\n",
        "  # Initialize the output array with shape (Batch_size, patch_size, num_patches)\n",
        "  output = np.empty((B, patch_size, num_patches))\n",
        "\n",
        "  for b_idx in range(B): # Iterate through each image in the batch\n",
        "    patch_count = 0\n",
        "    for i in range(output_h): # Iterate through the height of the output feature map\n",
        "      for j in range(output_w): # Iterate through the width of the output feature map\n",
        "        # Extract the current patch and flatten it, assuming (C, H, W) for image slice\n",
        "        patch = images[b_idx, :, i:i+kernel_h, j:j+kernel_w].flatten()\n",
        "        # Assign the flattened patch as a column vector in the output array\n",
        "        output[b_idx, :, patch_count] = patch\n",
        "        patch_count += 1\n",
        "  return output"
      ],
      "metadata": {
        "id": "PmMz-LwPON78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_kernels = np.ones(shape=(2, 3, 3, 3))\n",
        "test_images_col = im2col(test_images, test_kernels)\n",
        "print(f\"Im2col transformed shape : {test_images_col.shape}\")"
      ],
      "metadata": {
        "id": "hgrZFUr8Oa5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our manual implementation of the **im2col** function is working, I need to implement the convolution function. Meaning calculating the dot product between different parts of the image with kernels and extracting pattern information. While initializing the kernels, I will be using the **Kaiming (He)** Initialization which works well with **ReLU** activation function which I will use after the convolution processes. Also, it is better to include **im2col**, bias adding and reshaping in the convolution function so I will be doing that.\n",
        "\n",
        "Let's start with simple kernel and bias creation functions."
      ],
      "metadata": {
        "id": "qzJa2DOkX8Zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_kernels(amount, channels, h, w) :\n",
        "  kernels = np.empty(shape=(amount, channels, h, w)) # Create kernels array\n",
        "  # initialize all the kernels\n",
        "  for i in range(amount) :\n",
        "    kernels[i] = np.random.randn(channels, h, w) * np.sqrt(2/kernels[0].size)\n",
        "  return kernels"
      ],
      "metadata": {
        "id": "4Zkra4vSCkG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_biases(num_kernels):\n",
        "    return np.zeros(num_kernels)"
      ],
      "metadata": {
        "id": "c2glD1P1JJVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the newly created functions to create our test kernels and biases."
      ],
      "metadata": {
        "id": "pOqdbNn9MOEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_kernels = create_kernels(32, test_images.shape[1], 3, 3)\n",
        "test_biases = create_biases(test_kernels.shape[0])\n",
        "print(f\"Test Kernels Shape : {test_kernels.shape}\")\n",
        "print(f\"Test Biases Shape : {test_biases.shape}\")"
      ],
      "metadata": {
        "id": "RCc_5ftnfBO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the convolve function which will include **im2col** transformation."
      ],
      "metadata": {
        "id": "7ikDIjSjMW9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convolve(images, kernels, biases, padding='valid') :\n",
        "  # Extract images' and kernels' shapes\n",
        "  B, C_img, H_img, W_img = images.shape\n",
        "  num_filters, _, kernel_h, kernel_w = kernels.shape\n",
        "\n",
        "  # Apply padding if 'same' is specified\n",
        "  if padding == 'same':\n",
        "    # Calculate padding amount for 'same' padding\n",
        "    pad_h = (kernel_h - 1) // 2 # 1\n",
        "    pad_w = (kernel_w - 1) // 2 # 1\n",
        "    # Apply padding to images. Padding tuples must match (B, C_img, H_img, W_img)\n",
        "    images_processed = np.pad(images, ((0,0), (0,0), (pad_h, pad_h), (pad_w, pad_w)), mode='constant')\n",
        "  else: # 'valid' or no padding\n",
        "    images_processed = images\n",
        "\n",
        "  # Extract dimensions from potentially padded images\n",
        "  _, _, H_proc, W_proc = images_processed.shape\n",
        "\n",
        "  # Calculate output dimensions based on processed images\n",
        "  output_h = H_proc - kernel_h + 1\n",
        "  output_w = W_proc - kernel_w + 1\n",
        "\n",
        "  # Apply im2col transformation to the processed (potentially padded) images\n",
        "  col_images = im2col(images_processed, kernels)  # (B, patch_size, num_patches)\n",
        "\n",
        "  # Flatten kernels\n",
        "  flattened_kernels = kernels.reshape(num_filters, -1)  # (num_filters, patch_size)\n",
        "\n",
        "  # Convolve (matrix multiply)\n",
        "  output = np.empty((B, num_filters, output_h * output_w))\n",
        "  for i in range(B):\n",
        "    # The dot product will now be (num_filters, patch_size) @ (patch_size, num_patches)\n",
        "    # Resulting in (num_filters, num_patches), which is (5, 900) in this case.\n",
        "    output[i] = np.dot(flattened_kernels, col_images[i])\n",
        "\n",
        "  # Add biases\n",
        "  for i in range(num_filters):\n",
        "    output[:, i, :] += biases[i]\n",
        "\n",
        "  # Reshape to spatial format\n",
        "  output = output.reshape(B, num_filters, output_h, output_w)\n",
        "\n",
        "  return output"
      ],
      "metadata": {
        "id": "fNP-XeOPlrgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time to test that as well."
      ],
      "metadata": {
        "id": "q8AC4Rr7MdMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convolved_test_valid = convolve(test_images, test_kernels, test_biases, padding='valid')\n",
        "print(f\"Convolved output shape (valid padding): {convolved_test_valid.shape}\")\n",
        "\n",
        "# Test with 'same' padding\n",
        "convolved_test_same = convolve(test_images, test_kernels, test_biases, padding='same')\n",
        "print(f\"Convolved output shape (same padding): {convolved_test_same.shape}\")"
      ],
      "metadata": {
        "id": "kp0h8Kdam7z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The convolution gave us feature maps which have the correct shape. The next step will be applying an activation function on this. I will be using **ReLU** as I have mentioned before."
      ],
      "metadata": {
        "id": "DXtjbGOqrFsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ReLU(data) :\n",
        "  return np.maximum(0, data)"
      ],
      "metadata": {
        "id": "YtSeCiydMtkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run a quick test."
      ],
      "metadata": {
        "id": "sPTLPtSONXt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Min before ReLU : {np.min(convolved_test_same)}\")\n",
        "convolved_test_same = ReLU(convolved_test_same)\n",
        "print(f\"Min after ReLU : {np.min(convolved_test_same)}\")"
      ],
      "metadata": {
        "id": "qkUvwvWANJPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReLU is done as well which was probably the easiest part yet... Now I need to implement something called a **Max Pooling** which will decrease the input feature maps while retaining the most information. I will use 2x2 windows with a stride of 2 which is the most common setup for a pooling layer."
      ],
      "metadata": {
        "id": "7y5VTTJnNZUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_pooling(data, window_size) :\n",
        "  B, C, H, W = data.shape # Get dimensions\n",
        "  output_h = H // window_size # reduce output H\n",
        "  output_w = W // window_size # reduce output W\n",
        "\n",
        "  output = np.empty(shape=(B, C, output_h, output_w)) # prepare output array\n",
        "  max_indices = np.empty(shape=(B, C, output_h, output_w, 2), dtype=int)  # Store (h, w) indices\n",
        "\n",
        "  for b in range(B) : # for every image\n",
        "    for c in range(C) : # for every channel\n",
        "      for i in range(output_h) :\n",
        "        for j in range(output_w) :\n",
        "          # Calculate the start and end indices for the current window\n",
        "          h_start = i * window_size\n",
        "          h_end = h_start + window_size\n",
        "          w_start = j * window_size\n",
        "          w_end = w_start + window_size\n",
        "          window = data[b, c, h_start:h_end, w_start:w_end]\n",
        "          output[b, c, i, j] = np.max(window)\n",
        "\n",
        "          # Find where the max came from (relative to window)\n",
        "          max_pos = np.unravel_index(np.argmax(window), window.shape)\n",
        "          # Store absolute position\n",
        "          max_indices[b, c, i, j, 0] = h_start + max_pos[0]\n",
        "          max_indices[b, c, i, j, 1] = w_start + max_pos[1]\n",
        "\n",
        "  return output, max_indices"
      ],
      "metadata": {
        "id": "ILI2jrcJK97s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Max pooling has been implemented as well, let's test if it correctly reduces the size of our images."
      ],
      "metadata": {
        "id": "DhMtj2OgQ-aX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pooled_test, _ = apply_pooling(convolved_test_same, 2)\n",
        "print(f\"Max Pooled : {pooled_test.shape}\")"
      ],
      "metadata": {
        "id": "yTpQyaXDLgIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes! With a window size of 2, the height and weight of the images got halved which is perfect. For a realistic test, let's run the convolution and pooling layers once more to see if anything breaks as I mentioned that I would use two of each at the start."
      ],
      "metadata": {
        "id": "23djSRpeREk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create new test kernels and biases\n",
        "test_kernels_2 = create_kernels(64, pooled_test.shape[1], 3, 3)\n",
        "test_biases_2 = create_biases(test_kernels_2.shape[0])\n",
        "\n",
        "# Run convolution again\n",
        "convolved_test_2 = convolve(pooled_test, test_kernels_2, test_biases_2, padding='same')\n",
        "print(f\"Convolved for the second time : {convolved_test_2.shape}\")\n",
        "\n",
        "# Run pooling again\n",
        "pooled_test_2, _ = apply_pooling(convolved_test_2, 2)\n",
        "print(f\"Pooled for the second time : {pooled_test_2.shape}\")"
      ],
      "metadata": {
        "id": "GKfkzftXjv4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay the outputs' shapes look exactly as I wanted as well. Now I need a flattening function for these images to feed them into the dense (Fully Connected Neural Network) layer which I will also implement afterwards."
      ],
      "metadata": {
        "id": "d6ZHsNG8mj9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(data) :\n",
        "  B = data.shape[0]\n",
        "  data = np.reshape(data, (B, -1))\n",
        "  return data"
      ],
      "metadata": {
        "id": "JRNKDNQcm2SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flattened_test = flatten(pooled_test_2)\n",
        "print(f\"Flattened data shape : {flattened_test.shape}\")"
      ],
      "metadata": {
        "id": "Pl5UWKFqnGAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nice. The output's shape before the first Dense layer corresponds to 4096 inputs for every sample. For the last dense layer, I need another activation function before the loss calculation. For this project, I will use the **Softmax** activation function and the **Cross Entropy** loss functions. Now let's implement them."
      ],
      "metadata": {
        "id": "lWWlsCF5nTDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(logits) :\n",
        "  if logits.ndim == 1:  # Single sample\n",
        "    shifted = logits - np.max(logits) # Subtract max for numerical stability (prevents overflow)\n",
        "    exp_vals = np.exp(shifted)\n",
        "    return exp_vals / np.sum(exp_vals)\n",
        "  else:  # Batch\n",
        "    shifted = logits - np.max(logits, axis=1, keepdims=True) # Subtract max for numerical stability (prevents overflow)\n",
        "    exp_vals = np.exp(shifted)\n",
        "    return exp_vals / np.sum(exp_vals, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "1T38OqUW0IKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds = softmax(np.random.rand(10))\n",
        "print(f\"Probabilities : {test_preds}\")\n",
        "print(f\"Sum of probabilities : {np.sum(test_preds)}\")"
      ],
      "metadata": {
        "id": "YVD7njMf1Nts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, with dummy data, it can be seen that the method correctly converts randomly initialized logits into probabilities and their sum equals to 1. (Well, there might be microscopic errors). Now it is time to implement the **Cross Entropy Loss** function. And for the cross entropy loss to work, I will have to turn true labels into one-hot encoded versions."
      ],
      "metadata": {
        "id": "UUb87iso3Fyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(labels, num_classes) :\n",
        "  labels = np.array(labels)\n",
        "  one_hot = np.zeros((labels.shape[0], num_classes))\n",
        "  one_hot[np.arange(labels.shape[0]), labels] = 1\n",
        "  return one_hot"
      ],
      "metadata": {
        "id": "Kr92pInF77xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create encoded labels\n",
        "y_train_oh = one_hot_encode(y_train, len(dataset_full.classes))\n",
        "y_val_oh = one_hot_encode(y_val, len(dataset_full.classes))\n",
        "y_test_oh = one_hot_encode(y_test, len(dataset_full.classes))\n",
        "\n",
        "# Pick random index\n",
        "rand_idx = random.randint(0, len(y_train) + 1)\n",
        "\n",
        "print(f\"Random record of original labels : Index {rand_idx}, Original {y_train[rand_idx]}, Encoded {y_train_oh[rand_idx]} \")"
      ],
      "metadata": {
        "id": "rv3q16iZ9B12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that one-hot encoding is implemented, I can create the **Cross Entropy** Loss function as well."
      ],
      "metadata": {
        "id": "9SIP7QL-9A8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(preds, true_labels) :\n",
        "  # Add small epsilon to prevent log(0)\n",
        "  epsilon = 1e-15\n",
        "  preds = np.clip(preds, epsilon, 1 - epsilon)\n",
        "\n",
        "  # Calculate loss\n",
        "  batch_size = preds.shape[0] if preds.ndim > 1 else 1\n",
        "  return -np.sum(true_labels * np.log(preds)) / batch_size"
      ],
      "metadata": {
        "id": "q2SWisIo4tDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = cross_entropy_loss(test_preds, y_train_oh[1])\n",
        "print(f\"Total loss : {test_loss}\")"
      ],
      "metadata": {
        "id": "7WimBx5B60uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, time to work on the Dense layers."
      ],
      "metadata": {
        "id": "O_99dyns_ZHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dense() :\n",
        "  def __init__(self, input_amount, output_amount) :\n",
        "    self.input_amount = input_amount\n",
        "    self.output_amount = output_amount\n",
        "    self.weights = np.random.randn(input_amount, output_amount) * np.sqrt(2.0 / input_amount) # He Initialization\n",
        "    self.biases = np.zeros(shape=(output_amount))\n",
        "    print(f\"Created Dense Layer with {input_amount} Inputs and {output_amount} Outputs\")\n",
        "    print(f\"Weights shape : {self.weights.shape}\")\n",
        "    print(f\"Biases shape : {self.biases.shape}\\n\")\n",
        "\n",
        "  def forward(self, data, activation='relu') :\n",
        "    # Ensure data is 2D\n",
        "    if data.ndim == 1:\n",
        "      data = data.reshape(1, -1)\n",
        "\n",
        "    cache = {}\n",
        "    cache['input'] = data\n",
        "\n",
        "    # Linear transformation\n",
        "    weighted = np.dot(data, self.weights)\n",
        "    biased = weighted + self.biases\n",
        "\n",
        "    cache['input'] = data\n",
        "    cache['weighted'] = weighted\n",
        "    cache['biased'] = biased\n",
        "\n",
        "    # Apply activation\n",
        "    if activation == 'relu':\n",
        "      output = ReLU(biased)\n",
        "    elif activation == 'softmax':\n",
        "      output = softmax(biased)  # No ReLU before softmax!\n",
        "    else:  # 'none'\n",
        "      output = biased\n",
        "\n",
        "    cache['output'] = output\n",
        "\n",
        "    return output, cache"
      ],
      "metadata": {
        "id": "83LHPZQPqZYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for a quick test."
      ],
      "metadata": {
        "id": "tiaCrwZJJ0WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize first dense layer\n",
        "dense_1 = Dense(flattened_test.shape[1], 128)\n",
        "\n",
        "# Run forward pass once to test\n",
        "logits_1, cache_1 = dense_1.forward(flattened_test, 'relu')\n",
        "\n",
        "for key, val in cache_1.items() : # Print output shapes\n",
        "  print(f\"{key} shape : {val.shape}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Initialize second (last) dense layer\n",
        "dense_2 = Dense(128, len(dataset_full.classes))\n",
        "\n",
        "# Run forward pass on the second (last) layer\n",
        "preds_2, cache_2 = dense_2.forward(cache_1['output'], 'softmax')\n",
        "\n",
        "for key, val in cache_2.items() : # Print output shapes\n",
        "  print(f\"{key} shape : {val.shape}\")\n",
        "\n",
        "print(f\"Test sample predictions sum : {np.sum(cache_2['output'][0])}\") # Should be 1 (or very very close)"
      ],
      "metadata": {
        "id": "ZnbNGcv7rxEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Complete Forward Propagation\n",
        "As all the mathematical components were implemented, it is finally time to implement a complete forward propagation method."
      ],
      "metadata": {
        "id": "WaU3dVhbJ12q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn_np_forward(data, labels, kernels_1, biases_1, kernels_2, biases_2,\n",
        "                   window_size, dense_layer_1, dense_layer_2):\n",
        "    cache = {}\n",
        "\n",
        "    # Save original input\n",
        "    cache['input'] = data\n",
        "\n",
        "    # === Conv Block 1 ===\n",
        "    cache['kernels_1'] = kernels_1\n",
        "    cache['biases_1'] = biases_1\n",
        "\n",
        "    conv1_out = convolve(data, kernels_1, biases_1, 'same')\n",
        "    cache['conv1_out'] = conv1_out\n",
        "\n",
        "    relu1_out = ReLU(conv1_out)\n",
        "    cache['relu1_out'] = relu1_out\n",
        "\n",
        "    pool1_out, pool1_indices = apply_pooling(relu1_out, window_size)\n",
        "    cache['pool1_out'] = pool1_out\n",
        "    cache['pool1_indices'] = pool1_indices  # CRITICAL for backprop!\n",
        "\n",
        "    # === Conv Block 2 ===\n",
        "    cache['kernels_2'] = kernels_2\n",
        "    cache['biases_2'] = biases_2\n",
        "\n",
        "    conv2_out = convolve(pool1_out, kernels_2, biases_2, 'same')\n",
        "    cache['conv2_out'] = conv2_out\n",
        "\n",
        "    relu2_out = ReLU(conv2_out)\n",
        "    cache['relu2_out'] = relu2_out\n",
        "\n",
        "    pool2_out, pool2_indices = apply_pooling(relu2_out, window_size)\n",
        "    cache['pool2_out'] = pool2_out\n",
        "    cache['pool2_indices'] = pool2_indices\n",
        "\n",
        "    # === Dense Layers ===\n",
        "    flattened = flatten(pool2_out)\n",
        "    cache['flattened'] = flattened\n",
        "    cache['flattened_shape'] = pool2_out.shape  # To reshape back during backprop\n",
        "\n",
        "    dense1_out, dense1_cache = dense_layer_1.forward(flattened, 'relu')\n",
        "    cache['dense1_cache'] = dense1_cache\n",
        "\n",
        "    preds, dense2_cache = dense_layer_2.forward(dense1_out, 'softmax')\n",
        "    cache['dense2_cache'] = dense2_cache\n",
        "    cache['preds'] = preds\n",
        "\n",
        "    # === Loss ===\n",
        "    avg_loss = cross_entropy_loss(preds, labels)\n",
        "    cache['labels'] = labels\n",
        "    cache['loss'] = avg_loss\n",
        "\n",
        "    return preds, cache, avg_loss"
      ],
      "metadata": {
        "id": "qU1_KM32KT6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I can finally run the forward propagation and basically prediction as well for an untrained model."
      ],
      "metadata": {
        "id": "0D9jn6vrzGDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Initialize all layers ONCE ===\n",
        "\n",
        "# Conv layer 1: 3 channels → 32 filters\n",
        "kernels_1 = create_kernels(32, 3, 3, 3)\n",
        "biases_1 = create_biases(32)\n",
        "\n",
        "# Conv layer 2: 32 channels → 64 filters\n",
        "kernels_2 = create_kernels(64, 32, 3, 3)\n",
        "biases_2 = create_biases(64)\n",
        "\n",
        "# Dense layers (already initialized)\n",
        "dense_layer_1 = Dense(4096, 128)\n",
        "dense_layer_2 = Dense(128, 10)\n",
        "\n",
        "# === Run forward pass ===\n",
        "test_preds, cache, loss = cnn_np_forward(\n",
        "    X_train_scaled[:100],\n",
        "    y_train_oh[:100],\n",
        "    kernels_1, biases_1,\n",
        "    kernels_2, biases_2,\n",
        "    2,  # window_size\n",
        "    dense_layer_1,\n",
        "    dense_layer_2\n",
        ")\n",
        "\n",
        "print(f\"Loss: {loss:.4f}\")\n",
        "print(f\"Predicted class: {dataset_full.classes[np.argmax(test_preds[0])]}\")\n",
        "print(f\"True class: {dataset_full.classes[y_train[0]]}\")"
      ],
      "metadata": {
        "id": "3CZ0X2CVzTZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obviously as the model hasn't been trained, the prediction is random. Which is why we train models in the first place.\n",
        "# 4 - Backward Propagation\n",
        "After forward propagation (prediction), we get a loss value that tells us how wrong the prediction was. By using that value, we update our weights and biases so that the model learns and does better the next time. To calculate how much each weight/bias contributed to the loss (and to decide how much each of them should change) we implement something called \"**Backward Propagation**\". This is because the process is done in the reverse order compared to Forward Propagation."
      ],
      "metadata": {
        "id": "XZ6ZYkgO2skb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_backward(dL_doutput, input_data, kernels, padding='same'):\n",
        "  # Extract input data, kernel and output shape\n",
        "  B, C_in, H_in, W_in = input_data.shape\n",
        "  num_filters, _, k_h, k_w = kernels.shape\n",
        "  _, _, H_out, W_out = dL_doutput.shape\n",
        "\n",
        "  # Apply same padding to input if needed\n",
        "  if padding == 'same':\n",
        "    pad_h = (k_h - 1) // 2\n",
        "    pad_w = (k_w - 1) // 2\n",
        "    input_padded = np.pad(input_data, ((0,0), (0,0), (pad_h, pad_h), (pad_w, pad_w)), mode='constant')\n",
        "  else:\n",
        "    input_padded = input_data\n",
        "    pad_h = pad_w = 0\n",
        "\n",
        "  # Gradient w.r.t Biases\n",
        "  # Sum gradient across batch and spatial dimensions\n",
        "  dL_dbiases = np.sum(dL_doutput, axis=(0, 2, 3))  # (num_filters,)\n",
        "\n",
        "  # Gradient w.r.t Kernels\n",
        "  dL_dkernels = np.zeros_like(kernels)  # (num_filters, C_in, k_h, k_w)\n",
        "\n",
        "  # For each filter\n",
        "  for f in range(num_filters):\n",
        "    # For each input channel\n",
        "    for c in range(C_in):\n",
        "      # For each position in kernel\n",
        "      for i in range(k_h):\n",
        "        for j in range(k_w):\n",
        "          # Extract the region of input that this kernel position saw\n",
        "          input_region = input_padded[:, c, i:i+H_out, j:j+W_out]  # (B, H_out, W_out)\n",
        "          # Gradient for this kernel weight is correlation with gradient\n",
        "          dL_dkernels[f, c, i, j] = np.sum(input_region * dL_doutput[:, f, :, :]) / B  # Average over batch\n",
        "\n",
        "  # Gradient w.r.t Input\n",
        "  # Need to \"spread\" gradient back through convolution\n",
        "  _, _, H_padded, W_padded = input_padded.shape\n",
        "  dL_dinput_padded = np.zeros((B, C_in, H_padded, W_padded))\n",
        "\n",
        "  # For each sample in batch\n",
        "  for b in range(B):\n",
        "    # For each output channel (filter)\n",
        "    for f in range(num_filters):\n",
        "      # For each output position\n",
        "      for i in range(H_out):\n",
        "        for j in range(W_out):\n",
        "          # The gradient at this output position affects a k_h x k_w region of input\n",
        "          # Add contribution from this filter\n",
        "          dL_dinput_padded[b, :, i:i+k_h, j:j+k_w] += (kernels[f] * dL_doutput[b, f, i, j])\n",
        "\n",
        "  # Remove padding if it was added\n",
        "  if padding == 'same':\n",
        "    dL_dinput = dL_dinput_padded[:, :, pad_h:-pad_h, pad_w:-pad_w]\n",
        "  else:\n",
        "    dL_dinput = dL_dinput_padded\n",
        "\n",
        "  return dL_dkernels, dL_dbiases, dL_dinput"
      ],
      "metadata": {
        "id": "ovXXv4NOtCZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn_np_backward(cache, dense_layer_1, dense_layer_2, window_size=2):\n",
        "  gradients = {}\n",
        "\n",
        "  # Softmax + Cross-Entropy Gradient\n",
        "  # Beautiful simplified form: preds - true_labels\n",
        "  dL_dsoftmax = cache['preds'] - cache['labels']  # (B, 10)\n",
        "\n",
        "  # Dense Layer 2 Backward\n",
        "  # Gradient w.r.t weights: input.T @ gradient\n",
        "  gradients['dense2_weights'] = np.dot(cache['dense2_cache']['input'].T, dL_dsoftmax) / dL_dsoftmax.shape[0]  # Average over batch: (128, 10)\n",
        "\n",
        "  # Gradient w.r.t biases: sum over batch\n",
        "  gradients['dense2_biases'] = np.sum(dL_dsoftmax, axis=0) / dL_dsoftmax.shape[0]  # (10,)\n",
        "\n",
        "  # Gradient w.r.t input: gradient @ weights.T\n",
        "  dL_ddense2_input = np.dot(dL_dsoftmax, dense_layer_2.weights.T)  # (B, 128)\n",
        "\n",
        "  # ReLU Backward (between Dense layers)\n",
        "  # Gradient passes through where input > 0, zero elsewhere\n",
        "  dL_drelu = dL_ddense2_input * (cache['dense1_cache']['biased'] > 0)  # (B, 128)\n",
        "\n",
        "  # Dense Layer 1 Backward\n",
        "  # Gradient w.r.t weights\n",
        "  gradients['dense1_weights'] = np.dot(cache['dense1_cache']['input'].T, dL_drelu) / dL_drelu.shape[0]  # (4096, 128)\n",
        "\n",
        "  # Gradient w.r.t biases\n",
        "  gradients['dense1_biases'] = np.sum(dL_drelu, axis=0) / dL_drelu.shape[0]  # (128,)\n",
        "\n",
        "  # Gradient w.r.t input\n",
        "  dL_ddense1_input = np.dot(dL_drelu, dense_layer_1.weights.T)  # (B, 4096)\n",
        "\n",
        "  # Unflatten (Reshape) Backward\n",
        "  # Just reshape gradient back to 4D: (B, 4096) -> (B, 64, 8, 8)\n",
        "  dL_dpool2_out = dL_ddense1_input.reshape(cache['flattened_shape'])  # (B, 64, 8, 8)\n",
        "\n",
        "  # Max Pooling 2 Backward\n",
        "  # Create zeros with shape before pooling: (B, 64, 16, 16)\n",
        "  B, C, pool_H, pool_W = dL_dpool2_out.shape\n",
        "  unpooled_H = pool_H * window_size\n",
        "  unpooled_W = pool_W * window_size\n",
        "  dL_drelu2_out = np.zeros((B, C, unpooled_H, unpooled_W))\n",
        "\n",
        "  # Route gradient to max positions\n",
        "  pool2_indices = cache['pool2_indices']\n",
        "  for b in range(B):\n",
        "    for c in range(C):\n",
        "      for i in range(pool_H):\n",
        "        for j in range(pool_W):\n",
        "          # Get where the max came from\n",
        "          max_h = pool2_indices[b, c, i, j, 0]\n",
        "          max_w = pool2_indices[b, c, i, j, 1]\n",
        "          # Place gradient at that position\n",
        "          dL_drelu2_out[b, c, max_h, max_w] = dL_dpool2_out[b, c, i, j]\n",
        "\n",
        "  # ReLU 2 Backward\n",
        "  # Gradient passes through where conv2 output > 0\n",
        "  dL_dconv2_out = dL_drelu2_out * (cache['conv2_out'] > 0)  # (B, 64, 16, 16)\n",
        "\n",
        "  # Convolution 2 Backward\n",
        "  # This is the hardest part!\n",
        "  dL_dkernels2, dL_dbiases2, dL_dpool1_out = conv_backward(dL_dconv2_out, cache['pool1_out'], cache['kernels_2'], padding='same')\n",
        "\n",
        "  gradients['kernels_2'] = dL_dkernels2\n",
        "  gradients['biases_2'] = dL_dbiases2\n",
        "\n",
        "  # Max Pooling 1 Backward\n",
        "  B, C, pool_H, pool_W = dL_dpool1_out.shape\n",
        "  unpooled_H = pool_H * window_size\n",
        "  unpooled_W = pool_W * window_size\n",
        "  dL_drelu1_out = np.zeros((B, C, unpooled_H, unpooled_W))\n",
        "\n",
        "  pool1_indices = cache['pool1_indices']\n",
        "  for b in range(B):\n",
        "    for c in range(C):\n",
        "      for i in range(pool_H):\n",
        "        for j in range(pool_W):\n",
        "          max_h = pool1_indices[b, c, i, j, 0]\n",
        "          max_w = pool1_indices[b, c, i, j, 1]\n",
        "          dL_drelu1_out[b, c, max_h, max_w] = dL_dpool1_out[b, c, i, j]\n",
        "\n",
        "  # ReLU 1 Backward\n",
        "  dL_dconv1_out = dL_drelu1_out * (cache['conv1_out'] > 0)  # (B, 32, 32, 32)\n",
        "\n",
        "  # Convolution 1 Backward\n",
        "  dL_dkernels1, dL_dbiases1, dL_dinput = conv_backward(dL_dconv1_out, cache['input'], cache['kernels_1'], padding='same')\n",
        "\n",
        "  gradients['kernels_1'] = dL_dkernels1\n",
        "  gradients['biases_1'] = dL_dbiases1\n",
        "\n",
        "  return gradients"
      ],
      "metadata": {
        "id": "2orKfl9BggB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run backward pass\n",
        "gradients = cnn_np_backward(cache, dense_layer_1, dense_layer_2, window_size=2)\n",
        "\n",
        "# Check gradient shapes\n",
        "print(\"\\n=== Gradient Shapes ===\")\n",
        "print(f\"Dense2 weights gradient: {gradients['dense2_weights'].shape} (should be {dense_layer_2.weights.shape})\")\n",
        "print(f\"Dense2 biases gradient: {gradients['dense2_biases'].shape} (should be {dense_layer_2.biases.shape})\")\n",
        "print(f\"Dense1 weights gradient: {gradients['dense1_weights'].shape} (should be {dense_layer_1.weights.shape})\")\n",
        "print(f\"Dense1 biases gradient: {gradients['dense1_biases'].shape} (should be {dense_layer_1.biases.shape})\")\n",
        "print(f\"Kernels2 gradient: {gradients['kernels_2'].shape} (should be {kernels_2.shape})\")\n",
        "print(f\"Biases2 gradient: {gradients['biases_2'].shape} (should be {biases_2.shape})\")\n",
        "print(f\"Kernels1 gradient: {gradients['kernels_1'].shape} (should be {kernels_1.shape})\")\n",
        "print(f\"Biases1 gradient: {gradients['biases_1'].shape} (should be {biases_1.shape})\")"
      ],
      "metadata": {
        "id": "M891l_MzidaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 - Training Loop\n",
        "A training loop is comprised of multiple forward and backward propagations to get the model to predict as best as possible. Before the actual loop though, I would like to combine the methods I created until now to build a definitive CNN Numpy Model class which would make things a lot easier. This model will have a proper initialization, forward and backward propagation methods and a parameter update function."
      ],
      "metadata": {
        "id": "QuojJu5010CP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_model() :\n",
        "  def __init__(self, kernel_count, kernel_h, kernel_w, kernel_c, window_size) :\n",
        "    # Initialize 2 kernels\n",
        "    self.kernels_1 = create_kernels(kernel_count, kernel_c, kernel_h, kernel_w)\n",
        "    self.kernels_2 = create_kernels(kernel_count*2, kernel_count, kernel_h, kernel_w)\n",
        "    # Initialize biases for the kernels\n",
        "    self.biases_1 = create_biases(kernel_count)\n",
        "    self.biases_2 = create_biases(kernel_count*2)\n",
        "    # Determine pooling window size\n",
        "    self.window_size = window_size\n",
        "    # Initialize 2 Dense Layers\n",
        "    self.dense_1 = Dense(4096, 128)\n",
        "    self.dense_2 = Dense(128, len(dataset_full.classes))\n",
        "    print(f\"Created CNN Model!\")\n",
        "\n",
        "  def forward(self, X, y) :\n",
        "    preds, cache, avg_loss = cnn_np_forward(X, y, self.kernels_1, self.biases_1, self.kernels_2, self.biases_2, self.window_size, self.dense_1, self.dense_2)\n",
        "    return preds, cache, avg_loss\n",
        "\n",
        "  def backward(self, cache) :\n",
        "    gradients = cnn_np_backward(cache, self.dense_1, self.dense_2, self.window_size)\n",
        "    return gradients\n",
        "\n",
        "  def update_parameters(self, gradients, learning_rate) :\n",
        "    self.kernels_1 = self.kernels_1 - (gradients['kernels_1'] * learning_rate)\n",
        "    self.kernels_2 = self.kernels_2 - (gradients['kernels_2'] * learning_rate)\n",
        "    self.biases_1 = self.biases_1 - (gradients['biases_1'] * learning_rate)\n",
        "    self.biases_2 = self.biases_2 - (gradients['biases_2'] * learning_rate)\n",
        "    self.dense_1.weights = self.dense_1.weights - (gradients['dense1_weights'] * learning_rate)\n",
        "    self.dense_2.weights = self.dense_2.weights - (gradients['dense2_weights'] * learning_rate)\n",
        "    self.dense_1.biases = self.dense_1.biases - (gradients['dense1_biases'] * learning_rate)\n",
        "    self.dense_2.biases = self.dense_2.biases - (gradients['dense2_biases'] * learning_rate)\n",
        "\n",
        "  def predict(self, X) :\n",
        "    # Create dummy labels as we aren't gonna need actual labels here\n",
        "    dummy_labels = np.zeros((X.shape[0], len(dataset_full.classes)))\n",
        "    # Run forward propagation for predictions\n",
        "    preds, _, _ = self.forward(X, dummy_labels)\n",
        "    # Reverse OH encoding\n",
        "    return np.argmax(preds, axis=1)"
      ],
      "metadata": {
        "id": "TqFXYEMMIyNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_np = CNN_model(32, 3, 3, 3, 2)"
      ],
      "metadata": {
        "id": "U3gtwcaoNZC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally it is time for the training loop. Here I will initialize the parameters and then implement a function for model training."
      ],
      "metadata": {
        "id": "JdoFBmYqhttx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X_train, y_train, X_val, y_val, epochs, batch_size, learning_rate):\n",
        "    # Store losses for plotting\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    # Calculate batch count\n",
        "    batch_count = X_train.shape[0] // batch_size\n",
        "\n",
        "    print(f\"Training for {epochs} epochs with batch size {batch_size}\")\n",
        "    print(f\"Total batches per epoch: {batch_count}\")\n",
        "    print(f\"Learning rate: {learning_rate}\\n\")\n",
        "\n",
        "    for epoch in range(epochs):  # Remove tqdm here for cleaner output\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # TRAINING\n",
        "        # Shuffle training data to prevent order bias\n",
        "        random_indices = np.random.permutation(X_train.shape[0])\n",
        "        X_train_shuffled = X_train[random_indices]\n",
        "        y_train_shuffled = y_train[random_indices]\n",
        "\n",
        "        epoch_losses = []\n",
        "\n",
        "        # Mini batching with tqdm\n",
        "        for batch in tqdm(range(batch_count), desc=f\"Epoch {epoch+1}\", leave=False):\n",
        "            # Determine start and end points for a batch, then initialize the batch\n",
        "            start_idx = batch * batch_size\n",
        "            end_idx = start_idx + batch_size\n",
        "            X_batch = X_train_shuffled[start_idx:end_idx]\n",
        "            y_batch = y_train_shuffled[start_idx:end_idx]\n",
        "\n",
        "            # Run forward pass to predict, calculate losses and store intermediate values\n",
        "            preds, cache, avg_loss_train = model.forward(X_batch, y_batch)\n",
        "            epoch_losses.append(avg_loss_train)\n",
        "\n",
        "            # Run backward pass to calculate how much each parameter contributed to the error\n",
        "            gradients = model.backward(cache)\n",
        "\n",
        "            # Update weights with respect to the gradients\n",
        "            model.update_parameters(gradients, learning_rate)\n",
        "\n",
        "        # Calculate average training loss for this epoch\n",
        "        avg_train_loss = np.mean(epoch_losses)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # VALIDATION\n",
        "        print(\"Running validation...\")\n",
        "        val_batch_count = X_val.shape[0] // batch_size\n",
        "        val_epoch_losses = []\n",
        "\n",
        "        for batch in range(val_batch_count):\n",
        "            start_idx = batch * batch_size\n",
        "            end_idx = start_idx + batch_size\n",
        "            X_val_batch = X_val[start_idx:end_idx]\n",
        "            y_val_batch = y_val[start_idx:end_idx]\n",
        "\n",
        "            _, _, val_loss = model.forward(X_val_batch, y_val_batch)\n",
        "            val_epoch_losses.append(val_loss)\n",
        "\n",
        "        avg_val_loss = np.mean(val_epoch_losses)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # LOGGING\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "A5zZWkysiAZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, val_losses = train_model(model_np, X_train_scaled[:1000], y_train_oh[:1000], X_val_scaled[:1000], y_val_oh[:1000], 5, 64, 0.01)"
      ],
      "metadata": {
        "id": "TBihRLvVkrrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I also need a few more functions to run predictions (without altering the model parameters), and to evaluate the model performance."
      ],
      "metadata": {
        "id": "mwsdQ7Zr7Rej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(model, X, y_true_oh):\n",
        "  # Run predictions on the data\n",
        "  predictions = model.predict(X)\n",
        "  # Reverse OH encoding\n",
        "  y_true = np.argmax(y_true_oh, axis=1)\n",
        "  # Calculate accuracy\n",
        "  accuracy = np.mean(predictions == y_true) * 100\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "BNJZhBpA7cm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_val, y_val, batch_size):\n",
        "  # Calculate batch count\n",
        "  batch_count = X_val.shape[0] // batch_size\n",
        "  # Store losses\n",
        "  losses = []\n",
        "\n",
        "  print(\"Evaluating model...\")\n",
        "  for batch in tqdm(range(batch_count), desc=\"Evaluation\", leave=False):\n",
        "      # Determine batch indices\n",
        "      start_idx = batch * batch_size\n",
        "      end_idx = start_idx + batch_size\n",
        "      X_batch = X_val[start_idx:end_idx]\n",
        "      y_batch = y_val[start_idx:end_idx]\n",
        "\n",
        "      # Run forward pass and store loss value\n",
        "      _, _, loss = model.forward(X_batch, y_batch)\n",
        "      losses.append(loss)\n",
        "\n",
        "  # Calculate average loss\n",
        "  avg_loss = np.mean(losses)\n",
        "  # Calculate accuracy (use subset to avoid memory issues)\n",
        "  accuracy = calculate_accuracy(model, X_val[:1000], y_val[:1000])\n",
        "\n",
        "  return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "915kPGOI7YA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_accuracy(model_np, X_val_scaled[:1000], y_val_oh:1000)"
      ],
      "metadata": {
        "id": "JiL-qUDNxdzT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}